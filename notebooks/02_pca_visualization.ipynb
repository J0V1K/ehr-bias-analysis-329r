{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA Visualization of Word Embeddings by Race\n",
        "\n",
        "This notebook trains Word2Vec embeddings on discharge instructions from different racial groups and visualizes the top 100 most common words using PCA.\n",
        "\n",
        "## Analysis Steps:\n",
        "1. Load discharge instruction data\n",
        "2. Train Word2Vec models for each racial group\n",
        "3. Extract embeddings for top 100 words\n",
        "4. Reduce dimensionality with PCA\n",
        "5. Visualize word distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import custom data loader\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "from src.data_loader import load_for_analysis\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load discharge instructions\n",
        "df = load_for_analysis(\n",
        "    filepath='../data/merged_file_sample=100k_section=dischargeinstructions.csv',\n",
        "    sample_size=None,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(df)} records\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing (Fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Tokenize and clean text.\n",
        "    \n",
        "    FIXED: Previous version had lambda bug that overwrote tokens variable.\n",
        "    \"\"\"\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Convert to lowercase and remove \\x95 character\n",
        "    tokens = [re.sub(r'\\x95', \"\", token.lower()) for token in tokens if token != '\\x95']\n",
        "    \n",
        "    # Remove punctuation\n",
        "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Remove tokens with numbers\n",
        "    tokens = [token for token in tokens if not any(char.isdigit() for char in token)]\n",
        "    \n",
        "    # Remove empty tokens\n",
        "    tokens = [token for token in tokens if token]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Test the function\n",
        "sample_text = df['text'].iloc[0]\n",
        "print(\"Sample tokens:\", clean_text(sample_text)[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Word2Vec Models by Race\n",
        "\n",
        "We train separate Word2Vec models for each racial group to compare word usage patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('../results/PCA', exist_ok=True)\n",
        "\n",
        "for race in df['race_simplified'].unique():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing {race}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Filter by race and clean text\n",
        "    race_df = df[df['race_simplified'] == race].copy()\n",
        "    sentences = race_df['text'].apply(clean_text).tolist()\n",
        "    \n",
        "    print(f\"Number of documents: {len(sentences)}\")\n",
        "    print(f\"Sample tokens: {sentences[0][:30]}\")\n",
        "    \n",
        "    # Train Word2Vec model\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    model = Word2Vec(\n",
        "        sentences,\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=5,\n",
        "        sg=1,  # Skip-gram\n",
        "        workers=4\n",
        "    )\n",
        "    \n",
        "    # Save model\n",
        "    model.save(f'../results/PCA/{race}.wordvectors')\n",
        "    \n",
        "    # Get top 100 most common words\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    top_100_words = [word for word, _ in word_counts.most_common(100)]\n",
        "    \n",
        "    # Extract embeddings\n",
        "    embeddings = [model.wv[word] for word in top_100_words]\n",
        "    \n",
        "    # Reduce dimensionality with PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_embeddings = pca.fit_transform(embeddings)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.6)\n",
        "    \n",
        "    for i, word in enumerate(top_100_words):\n",
        "        plt.annotate(word, xy=(reduced_embeddings[i, 0], reduced_embeddings[i, 1]), \n",
        "                    fontsize=8, alpha=0.7)\n",
        "    \n",
        "    plt.title(f'Top 100 Word2Vec Embeddings - {race} Discharge Instructions')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot\n",
        "    plt.savefig(f'../results/PCA/{race}_embeddings.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Saved visualization to results/PCA/{race}_embeddings.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interpretation\n",
        "\n",
        "### What do these visualizations show?\n",
        "- Words closer together in the PCA space have similar contextual usage\n",
        "- Clusters reveal common medical topics and instruction patterns\n",
        "- Differences across racial groups may indicate:\n",
        "  - Variations in medical conditions\n",
        "  - Different communication styles\n",
        "  - Potential biases in care delivery\n",
        "\n",
        "### Limitations:\n",
        "- PCA reduction loses information (only 2D visualization of 100D space)\n",
        "- Word2Vec trained separately per group (not directly comparable)\n",
        "- Need Fighting Words analysis for statistical significance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}