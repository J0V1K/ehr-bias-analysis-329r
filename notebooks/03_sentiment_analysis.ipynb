{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis of Discharge Instructions\n",
        "\n",
        "This notebook performs sentiment analysis on hospital discharge instructions to identify potential differences in tone and sentiment across racial groups.\n",
        "\n",
        "## Analysis Overview:\n",
        "1. Load discharge instruction data\n",
        "2. Apply sentiment analysis using pre-trained models\n",
        "3. Compare sentiment distributions across demographic groups\n",
        "4. Assess statistical significance of differences\n",
        "\n",
        "## Model Used:\n",
        "- **DistilBERT** fine-tuned on SST-2 (Stanford Sentiment Treebank)\n",
        "- Binary classification: Positive/Negative sentiment\n",
        "- Confidence scores for each prediction\n",
        "\n",
        "## Key Question:\n",
        "Do discharge instructions show systematic differences in sentiment/tone across racial groups?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Sentiment analysis\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Custom data loader\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "from src.data_loader import load_for_analysis\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "tqdm.pandas()\n",
        "\n",
        "print(\"\u2713 Imports complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "\n",
        "We'll load discharge instructions using our standardized data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load discharge instructions\n",
        "# Note: Sentiment analysis is computationally expensive\n",
        "# Start with a sample for testing\n",
        "df = load_for_analysis(\n",
        "    filepath='../data/merged_file_sample=100k_section=dischargeinstructions.csv',\n",
        "    sample_size=1000,  # Start small for testing\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(df)} discharge instructions\")\n",
        "print(f\"\\nRace distribution:\")\n",
        "print(df['race_simplified'].value_counts())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Sentiment Analysis Model\n",
        "\n",
        "We use **DistilBERT** fine-tuned on SST-2 (Stanford Sentiment Treebank):\n",
        "- Faster than full BERT (40% smaller, 60% faster)\n",
        "- Maintains 97% of BERT's language understanding\n",
        "- Pre-trained on sentiment classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained sentiment analysis model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model_name,\n",
        "    tokenizer=model_name,\n",
        "    device=-1  # Use CPU (-1) or GPU (0)\n",
        ")\n",
        "\n",
        "print(\"\u2713 Model loaded successfully\")\n",
        "\n",
        "# Test the model\n",
        "test_text = \"You are recovering well and ready to go home.\"\n",
        "result = sentiment_pipeline(test_text)[0]\n",
        "print(f\"\\nTest: '{test_text}'\")\n",
        "print(f\"Sentiment: {result['label']}, Score: {result['score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Sentiment Analysis\n",
        "\n",
        "**Note:** This is computationally intensive. Processing time depends on:\n",
        "- Number of texts\n",
        "- Text length\n",
        "- Hardware (CPU vs GPU)\n",
        "\n",
        "Approximate timing:\n",
        "- 1,000 texts on CPU: ~10 minutes\n",
        "- 10,000 texts on CPU: ~100 minutes\n",
        "- With GPU: 5-10x faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment(text, max_length=512):\n",
        "    \"\"\"\n",
        "    Analyze sentiment of a text.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text\n",
        "        max_length: Maximum tokens (BERT limit is 512)\n",
        "    \n",
        "    Returns:\n",
        "        dict with label and score\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return {'label': 'NEUTRAL', 'score': 0.5}\n",
        "    \n",
        "    # Truncate if too long\n",
        "    if len(text) > max_length * 4:  # Rough estimate (chars -> tokens)\n",
        "        text = text[:max_length * 4]\n",
        "    \n",
        "    try:\n",
        "        result = sentiment_pipeline(text)[0]\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing text: {e}\")\n",
        "        return {'label': 'ERROR', 'score': 0.0}\n",
        "\n",
        "\n",
        "# Run sentiment analysis\n",
        "print(\"Running sentiment analysis...\")\n",
        "print(\"This may take several minutes...\\n\")\n",
        "\n",
        "# Apply to all texts\n",
        "sentiments = []\n",
        "for text in tqdm(df['text'], desc=\"Analyzing sentiment\"):\n",
        "    result = analyze_sentiment(text)\n",
        "    sentiments.append(result)\n",
        "\n",
        "# Add results to dataframe\n",
        "df['sentiment_label'] = [s['label'] for s in sentiments]\n",
        "df['sentiment_score'] = [s['score'] for s in sentiments]\n",
        "\n",
        "print(f\"\\n\u2713 Analyzed {len(df)} texts\")\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(df['sentiment_label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Sentiment Across Racial Groups\n",
        "\n",
        "Now we examine whether sentiment differs systematically by race."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate sentiment statistics by race\n",
        "sentiment_by_race = df.groupby('race_simplified').agg({\n",
        "    'sentiment_label': lambda x: (x == 'POSITIVE').sum() / len(x),\n",
        "    'sentiment_score': ['mean', 'std', 'count']\n",
        "})\n",
        "\n",
        "sentiment_by_race.columns = ['Positive_Pct', 'Mean_Score', 'Std_Score', 'Count']\n",
        "sentiment_by_race = sentiment_by_race.sort_values('Positive_Pct', ascending=False)\n",
        "\n",
        "print(\"Sentiment Analysis by Race:\")\n",
        "print(sentiment_by_race)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Positive percentage\n",
        "sentiment_by_race['Positive_Pct'].plot(\n",
        "    kind='bar',\n",
        "    ax=axes[0],\n",
        "    color='steelblue'\n",
        ")\n",
        "axes[0].set_title('Percentage of Positive Sentiment by Race')\n",
        "axes[0].set_xlabel('Race')\n",
        "axes[0].set_ylabel('% Positive')\n",
        "axes[0].set_ylim(0, 1)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Mean sentiment score\n",
        "sentiment_by_race['Mean_Score'].plot(\n",
        "    kind='bar',\n",
        "    ax=axes[1],\n",
        "    color='coral'\n",
        ")\n",
        "axes[1].set_title('Mean Sentiment Score by Race')\n",
        "axes[1].set_xlabel('Race')\n",
        "axes[1].set_ylabel('Mean Confidence Score')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/sentiment_by_race.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Statistical Significance Testing\n",
        "\n",
        "We use chi-square test to assess whether sentiment distributions differ across racial groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency, mannwhitneyu\n",
        "\n",
        "# Create contingency table\n",
        "contingency = pd.crosstab(\n",
        "    df['race_simplified'],\n",
        "    df['sentiment_label']\n",
        ")\n",
        "\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency)\n",
        "print()\n",
        "\n",
        "# Chi-square test\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
        "\n",
        "print(f\"Chi-Square Test Results:\")\n",
        "print(f\"  \u03c7\u00b2 = {chi2:.4f}\")\n",
        "print(f\"  p-value = {p_value:.6f}\")\n",
        "print(f\"  degrees of freedom = {dof}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(f\"\\n\u2713 Significant difference in sentiment across racial groups (p < 0.05)\")\n",
        "else:\n",
        "    print(f\"\\n\u2717 No significant difference in sentiment across racial groups (p \u2265 0.05)\")\n",
        "\n",
        "# Pairwise comparisons (example: WHITE vs BLACK)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Pairwise Comparison: WHITE vs BLACK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "white_scores = df[df['race_simplified'] == 'WHITE']['sentiment_score']\n",
        "black_scores = df[df['race_simplified'] == 'BLACK']['sentiment_score']\n",
        "\n",
        "if len(white_scores) > 0 and len(black_scores) > 0:\n",
        "    stat, p = mannwhitneyu(white_scores, black_scores, alternative='two-sided')\n",
        "    \n",
        "    print(f\"WHITE: Mean = {white_scores.mean():.3f}, Median = {white_scores.median():.3f}\")\n",
        "    print(f\"BLACK: Mean = {black_scores.mean():.3f}, Median = {black_scores.median():.3f}\")\n",
        "    print(f\"\\nMann-Whitney U test: U = {stat:.1f}, p = {p:.6f}\")\n",
        "    \n",
        "    if p < 0.05:\n",
        "        print(\"\u2713 Significant difference (p < 0.05)\")\n",
        "    else:\n",
        "        print(\"\u2717 No significant difference (p \u2265 0.05)\")\n",
        "else:\n",
        "    print(\"Insufficient data for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('../results/sentiment_analysis', exist_ok=True)\n",
        "\n",
        "# Save full results\n",
        "output_file = '../results/sentiment_analysis/sentiment_results.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Saved results to {output_file}\")\n",
        "\n",
        "# Save summary statistics\n",
        "summary_file = '../results/sentiment_analysis/sentiment_summary_by_race.csv'\n",
        "sentiment_by_race.to_csv(summary_file)\n",
        "print(f\"Saved summary to {summary_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interpretation and Limitations\n",
        "\n",
        "### What We're Measuring:\n",
        "- **Sentiment polarity:** Whether discharge instructions use positive vs. negative language\n",
        "- **Sentiment intensity:** Confidence of the sentiment classification\n",
        "\n",
        "### Important Caveats:\n",
        "\n",
        "1. **Medical Context:**\n",
        "   - Sentiment models are trained on general text (movie reviews, social media)\n",
        "   - Medical language has different conventions\n",
        "   - \"Negative\" sentiment may reflect medical necessity, not bias\n",
        "\n",
        "2. **Confounding Factors:**\n",
        "   - Disease severity differs across groups\n",
        "   - Socioeconomic factors affect health outcomes\n",
        "   - Cannot separate bias from legitimate medical differences\n",
        "\n",
        "3. **Model Limitations:**\n",
        "   - DistilBERT has 512 token limit (long texts are truncated)\n",
        "   - Binary classification (positive/negative) is simplistic\n",
        "   - May miss subtle tonal differences\n",
        "\n",
        "4. **Statistical Power:**\n",
        "   - Small sample sizes for some racial groups\n",
        "   - Effect sizes may be small\n",
        "   - Need large samples to detect differences\n",
        "\n",
        "### What This Adds to the Analysis:\n",
        "\n",
        "- **Complements Fighting Words:** While Fighting Words identifies *which* words differ, sentiment analysis assesses overall *tone*\n",
        "- **Hypothesis Generation:** Significant differences warrant deeper investigation\n",
        "- **Policy Relevance:** If systematic differences exist, suggests need for communication training\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. Run on full dataset (not just sample)\n",
        "2. Try domain-specific sentiment models (if available)\n",
        "3. Examine specific examples of high/low sentiment texts\n",
        "4. Control for disease severity and other clinical factors\n",
        "5. Qualitative analysis of flagged texts"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}